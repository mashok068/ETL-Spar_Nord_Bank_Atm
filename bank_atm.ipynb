{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_232-cloudera/jre\"\n",
    "os.environ[\"SPARK_HOME\"]=\"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '-- packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 --master local[2] pyspark-shell'\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-0-198.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_aws</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f81b824f790>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conf = SparkConf().set('spark.executor.extraJavaOptions', '-Dcom.amazonaws.services.s3.enableV4=true').set('spark.driver.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true').setAppName('pyspark_aws').setMaster('local[*]')\n",
    "#sc=SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.appName(\"pyspark_aws\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4', 'true')\n",
    "accessKeyId='AKIA5F4VPWZTWQJQL5EM'\n",
    "secretAccessKey='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'\n",
    "\n",
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key', accessKeyId)\n",
    "hadoopConf.set('fs.s3a.secret.key', secretAccessKey)\n",
    "hadoopConf.set('fs.s3a.endpoint', 's3-us-east-1.amazonaws.com')\n",
    "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "spark=SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileSchema = StructType([StructField('Year', IntegerType(),True),\n",
    "                        StructField('Month', StringType(),True),\n",
    "                        StructField('Day', IntegerType(),True),\n",
    "                        StructField('WeekDay', StringType(),True),\n",
    "                        StructField('Hour', IntegerType(),True),\n",
    "                        StructField('Atm_Status', StringType(),True),\n",
    "                        StructField('Atm_Id', StringType(),True),\n",
    "                        StructField('Atm_Manufacturer', StringType(),True),\n",
    "                        StructField('Atm_Location', StringType(),True),\n",
    "                        StructField('Atm_StreetName', StringType(),True),\n",
    "                        StructField('Atm_StreetNum', IntegerType(),True),\n",
    "                        StructField('Atm_Zipcode', IntegerType(),True),\n",
    "                        StructField('Atm_Lat', DoubleType(),True),\n",
    "                        StructField('Atm_Long', DoubleType(),True),\n",
    "                        StructField('Currency', StringType(),True),\n",
    "                        StructField('Card_type', StringType(),True),\n",
    "                        StructField('Transaction_Amt', IntegerType(),True), \n",
    "                        StructField('Service', StringType(),True),\n",
    "                        StructField('Message_Code', StringType(),True),\n",
    "                        StructField('Message_Text', StringType(),True),\n",
    "                        StructField('Weather_Lat', DoubleType(),True),\n",
    "                        StructField('Weather_Long', DoubleType(),True),\n",
    "                        StructField('Weather_CityId', IntegerType(),True),\n",
    "                        StructField('Weather_CityName', StringType(),True), \n",
    "                        StructField('Temp', DoubleType(),True),\n",
    "                        StructField('Pressure', IntegerType(),True), \n",
    "                        StructField('Humidity', IntegerType(),True), \n",
    "                        StructField('Wind_Speed', IntegerType(),True), \n",
    "                        StructField('Wind_Degree', IntegerType(),True), \n",
    "                        StructField('Rain_3h', DoubleType(),True), \n",
    "                        StructField('Clouds_All', IntegerType(),True), \n",
    "                        StructField('Weather_Id', IntegerType(),True), \n",
    "                        StructField('Weather_Main', StringType(),True), \n",
    "                        StructField('Weather_Description', StringType(),True),  \n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from csv to a DataFrame\n",
    "bank_df = spark.read.csv('spar_nord_bank_atm/part-m-00000', header = False, schema = fileSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- WeekDay: string (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Atm_Status: string (nullable = true)\n",
      " |-- Atm_Id: string (nullable = true)\n",
      " |-- Atm_Manufacturer: string (nullable = true)\n",
      " |-- Atm_Location: string (nullable = true)\n",
      " |-- Atm_StreetName: string (nullable = true)\n",
      " |-- Atm_StreetNum: integer (nullable = true)\n",
      " |-- Atm_Zipcode: integer (nullable = true)\n",
      " |-- Atm_Lat: double (nullable = true)\n",
      " |-- Atm_Long: double (nullable = true)\n",
      " |-- Currency: string (nullable = true)\n",
      " |-- Card_type: string (nullable = true)\n",
      " |-- Transaction_Amt: integer (nullable = true)\n",
      " |-- Service: string (nullable = true)\n",
      " |-- Message_Code: string (nullable = true)\n",
      " |-- Message_Text: string (nullable = true)\n",
      " |-- Weather_Lat: double (nullable = true)\n",
      " |-- Weather_Long: double (nullable = true)\n",
      " |-- Weather_CityId: integer (nullable = true)\n",
      " |-- Weather_CityName: string (nullable = true)\n",
      " |-- Temp: double (nullable = true)\n",
      " |-- Pressure: integer (nullable = true)\n",
      " |-- Humidity: integer (nullable = true)\n",
      " |-- Wind_Speed: integer (nullable = true)\n",
      " |-- Wind_Degree: integer (nullable = true)\n",
      " |-- Rain_3h: double (nullable = true)\n",
      " |-- Clouds_All: integer (nullable = true)\n",
      " |-- Weather_Id: integer (nullable = true)\n",
      " |-- Weather_Main: string (nullable = true)\n",
      " |-- Weather_Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-------------+-----------+-------+--------+--------+----------+---------------+----------+------------+------------+-----------+------------+--------------+----------------+------+--------+--------+----------+-----------+-------+----------+----------+------------+--------------------+\n",
      "|Year|  Month|Day|WeekDay|Hour|Atm_Status|Atm_Id|Atm_Manufacturer|Atm_Location|     Atm_StreetName|Atm_StreetNum|Atm_Zipcode|Atm_Lat|Atm_Long|Currency| Card_type|Transaction_Amt|   Service|Message_Code|Message_Text|Weather_Lat|Weather_Long|Weather_CityId|Weather_CityName|  Temp|Pressure|Humidity|Wind_Speed|Wind_Degree|Rain_3h|Clouds_All|Weather_Id|Weather_Main| Weather_Description|\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-------------+-----------+-------+--------+--------+----------+---------------+----------+------------+------------+-----------+------------+--------------+----------------+------+--------+--------+----------+-----------+-------+----------+----------+------------+--------------------+\n",
      "|2017|January|  1| Sunday|   0|    Active|     1|             NCR|  NÃƒÂ¦stved|        Farimagsvej|            8|       4700| 55.233|  11.763|     DKK|MasterCard|           5643|Withdrawal|        null|        null|      55.23|      11.761|       2616038|        Naestved|281.15|    1014|      87|         7|        260|  0.215|        92|       500|        Rain|          light rain|\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     2|             NCR|    Vejgaard|         Hadsundvej|           20|       9000| 57.043|    9.95|     DKK|MasterCard|           1764|Withdrawal|        null|        null|     57.048|       9.935|       2616235|  NÃƒÂ¸rresundby|280.64|    1020|      93|         9|        250|   0.59|        92|       500|        Rain|          light rain|\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     2|             NCR|    Vejgaard|         Hadsundvej|           20|       9000| 57.043|    9.95|     DKK|      VISA|           1891|Withdrawal|        null|        null|     57.048|       9.935|       2616235|  NÃƒÂ¸rresundby|280.64|    1020|      93|         9|        250|   0.59|        92|       500|        Rain|          light rain|\n",
      "|2017|January|  1| Sunday|   0|  Inactive|     3|             NCR|       Ikast|RÃƒÂ¥dhusstrÃƒÂ¦det|           12|       7430| 56.139|   9.154|     DKK|      VISA|           4166|Withdrawal|        null|        null|     56.139|       9.158|       2619426|           Ikast|281.15|    1011|     100|         6|        240|    0.0|        75|       300|     Drizzle|light intensity d...|\n",
      "|2017|January|  1| Sunday|   0|    Active|     4|             NCR|  Svogerslev|       BrÃƒÂ¸nsager|            1|       4000| 55.634|  12.018|     DKK|MasterCard|           5153|Withdrawal|        null|        null|     55.642|       12.08|       2614481|        Roskilde|280.61|    1014|      87|         7|        260|    0.0|        88|       701|        Mist|                mist|\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-------------+-----------+-------+--------+--------+----------+---------------+----------+------------+------------+-----------+------------+--------------+----------------+------+--------+--------+----------+-----------+-------+----------+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Month',\n",
       " 'Day',\n",
       " 'WeekDay',\n",
       " 'Hour',\n",
       " 'Atm_Status',\n",
       " 'Atm_Id',\n",
       " 'Atm_Manufacturer',\n",
       " 'Atm_Location',\n",
       " 'Atm_StreetName',\n",
       " 'Atm_StreetNum',\n",
       " 'Atm_Zipcode',\n",
       " 'Atm_Lat',\n",
       " 'Atm_Long',\n",
       " 'Currency',\n",
       " 'Card_type',\n",
       " 'Transaction_Amt',\n",
       " 'Service',\n",
       " 'Message_Code',\n",
       " 'Message_Text',\n",
       " 'Weather_Lat',\n",
       " 'Weather_Long',\n",
       " 'Weather_CityId',\n",
       " 'Weather_CityName',\n",
       " 'Temp',\n",
       " 'Pressure',\n",
       " 'Humidity',\n",
       " 'Wind_Speed',\n",
       " 'Wind_Degree',\n",
       " 'Rain_3h',\n",
       " 'Clouds_All',\n",
       " 'Weather_Id',\n",
       " 'Weather_Main',\n",
       " 'Weather_Description']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_df.select().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window  \n",
    "\n",
    "bank_df_indexed = bank_df.withColumn(\"Id\", row_number().over(Window.orderBy(monotonically_increasing_id())) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+---+-------+----+\n",
      "| Id|Year|  Month|Day|WeekDay|Hour|\n",
      "+---+----+-------+---+-------+----+\n",
      "|  0|2017|January|  1| Sunday|   0|\n",
      "|  1|2017|January|  1| Sunday|   0|\n",
      "|  2|2017|January|  1| Sunday|   0|\n",
      "|  3|2017|January|  1| Sunday|   0|\n",
      "|  4|2017|January|  1| Sunday|   0|\n",
      "+---+----+-------+---+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_df_indexed.select('Id', 'Year', 'Month', 'Day', 'WeekDay', 'Hour').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Card_Type Dimension\n",
    "bank_df.select('Card_Type').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|card_type_id|           card_type|\n",
      "+------------+--------------------+\n",
      "|           1|     Dankort - on-us|\n",
      "|           2|              CIRRUS|\n",
      "|           3|         HÃƒÂ¦vekort|\n",
      "|           4|                VISA|\n",
      "|           5|  Mastercard - on-us|\n",
      "|           6|             Maestro|\n",
      "|           7|Visa Dankort - on-us|\n",
      "|           8|        Visa Dankort|\n",
      "|           9|            VisaPlus|\n",
      "|          10|          MasterCard|\n",
      "|          11|             Dankort|\n",
      "|          12| HÃƒÂ¦vekort - on-us|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "card_type_dim = bank_df.select('Card_Type').distinct()\n",
    "card_type_dim = card_type_dim.withColumnRenamed('Card_Type', 'card_type')\n",
    "card_type_dim = card_type_dim.withColumn(\"card_type_id\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "card_type_dim = card_type_dim.select('card_type_id', 'card_type')\n",
    "card_type_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o126.save.\n: org.apache.hadoop.fs.s3a.AWSClientIOException: doesBucketExist on reshift-mashok-upgrad: com.amazonaws.SdkClientException: Unable to execute HTTP request: reshift-mashok-upgrad.s3-us-east-1.amazonaws.com: Name or service not known: Unable to execute HTTP request: reshift-mashok-upgrad.s3-us-east-1.amazonaws.com: Name or service not known\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:147)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:336)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:283)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2816)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:98)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2853)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2835)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:387)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:452)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:548)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:278)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: reshift-mashok-upgrad.s3-us-east-1.amazonaws.com: Name or service not known\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1069)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1035)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:742)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:716)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4221)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4168)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1306)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1263)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:324)\n\t... 23 more\nCaused by: java.net.UnknownHostException: reshift-mashok-upgrad.s3-us-east-1.amazonaws.com: Name or service not known\n\tat java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n\tat java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)\n\tat java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1193)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n\tat com.amazonaws.SystemDefaultDnsResolver.resolve(SystemDefaultDnsResolver.java:27)\n\tat com.amazonaws.http.DelegatingDnsResolver.resolve(DelegatingDnsResolver.java:38)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:111)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:353)\n\tat sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)\n\tat com.amazonaws.http.conn.$Proxy40.connect(Unknown Source)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:380)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1190)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1030)\n\t... 34 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-565ae5762948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcard_type_dim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3a://reshift-mashok-upgrad/card_type_dim/card_type_dim.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2//python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2//python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2//python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2//python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o126.save.\n: org.apache.hadoop.fs.s3a.AWSClientIOException: doesBucketExist on reshift-mashok-upgrad: com.amazonaws.SdkClientException: Unable to execute HTTP request: reshift-mashok-upgrad.s3-us-east-1.amazonaws.com: Name or service not known: Unable to execute HTTP request: reshift-mashok-upgrad.s3-us-east-1.amazonaws.com: Name or service not known\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:147)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:336)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:283)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2816)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:98)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2853)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2835)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:387)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:452)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:548)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:278)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: reshift-mashok-upgrad.s3-us-east-1.amazonaws.com: Name or service not known\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1069)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1035)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:742)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:716)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4221)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4168)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1306)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:1263)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.verifyBucketExists(S3AFileSystem.java:324)\n\t... 23 more\nCaused by: java.net.UnknownHostException: reshift-mashok-upgrad.s3-us-east-1.amazonaws.com: Name or service not known\n\tat java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)\n\tat java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)\n\tat java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324)\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1193)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n\tat com.amazonaws.SystemDefaultDnsResolver.resolve(SystemDefaultDnsResolver.java:27)\n\tat com.amazonaws.http.DelegatingDnsResolver.resolve(DelegatingDnsResolver.java:38)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:111)\n\tat com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:353)\n\tat sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)\n\tat com.amazonaws.http.conn.$Proxy40.connect(Unknown Source)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:380)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)\n\tat com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n\tat com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1190)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1030)\n\t... 34 more\n"
     ]
    }
   ],
   "source": [
    "card_type_dim.write.format('csv').option('header','true').save('s3a://reshift-mashok-upgrad/card_type_dim/card_type_dim.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Location Dimension\n",
    "bank_df.select('Atm_Location', 'Atm_StreetName','Atm_StreetNum','Atm_Zipcode','Atm_Lat', 'Atm_Long').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_dim = bank_df.select('Atm_Location', 'Atm_StreetName','Atm_StreetNum','Atm_Zipcode','Atm_Lat', 'Atm_Long').distinct()\n",
    "location_dim = location_dim.withColumnRenamed('Atm_Location', 'location').withColumnRenamed('Atm_StreetName', 'streetname').withColumnRenamed('Atm_StreetNum', 'street_number').withColumnRenamed('Atm_Zipcode', 'zipcode').withColumnRenamed('Atm_Lat', 'lat').withColumnRenamed('Atm_Long', 'lon')\n",
    "location_dim = location_dim.withColumn(\"location_id\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "location_dim = location_dim.select('location_id', 'location', 'streetname', 'street_number', 'zipcode','lat','lon')\n",
    "location_dim.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATM Dimension\n",
    "bank_df.select('Atm_Id','Atm_Manufacturer', 'Atm_location', 'Atm_StreetName','Atm_StreetNum','Atm_Zipcode','Atm_Lat', 'Atm_Long').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = bank_df.select('Atm_Id','Atm_Manufacturer', 'Atm_location', 'Atm_StreetName','Atm_StreetNum','Atm_Zipcode','Atm_Lat', 'Atm_Long').distinct()\n",
    "df2 = location_dim\n",
    "df1 = df1.alias('df1')\n",
    "df2 = df2.alias('df2')\n",
    "atm_dim = df1.join(df2, (df1['Atm_location'] == df2['location']) & \n",
    "                        (df1['Atm_StreetName'] == df2['streetname']) & \n",
    "                        (df1['Atm_StreetNum'] == df2['street_number']) & \n",
    "                        (df1['Atm_Zipcode'] == df2['zipcode']) & \n",
    "                        (df1['Atm_Lat'] == df2['lat']) & \n",
    "                        (df1['Atm_Long'] == df2['lon'])).select('df1.Atm_Id', 'df1.Atm_Manufacturer', 'df1.Atm_location', 'df2.location_id')\n",
    "\n",
    "atm_dim = atm_dim.withColumnRenamed('Atm_id', 'atm_number').withColumnRenamed('Atm_Manufacturer', 'atm_manufacturer').withColumnRenamed('Atm_Location', 'atm_location').withColumnRenamed('location_id', 'amt_location_id')\n",
    "atm_dim = atm_dim.withColumn(\"atm_id\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "atm_dim = atm_dim.select('atm_id', 'atm_number', 'atm_manufacturer', 'atm_location', 'amt_location_id')\n",
    "atm_dim.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atm_dim.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Date Dimension\n",
    "bank_df.select('Year','Month','Day','Hour').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, unix_timestamp, to_date, concat_ws\n",
    "bank_df = bank_df.withColumn(\"Month_Num\",from_unixtime(unix_timestamp(bank_df['Month'],'MMM'),'MM'))\n",
    "cols=[\"Year\",\"Month_Num\",\"Day\",\"Hour\"]\n",
    "bank_df = bank_df.withColumn(\"full_date_time\", to_date(concat_ws(\"-\", *cols),\"yyyy-MM-dd\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----+--------+---+----+---------+\n",
      "|date_id|     full_date_time|year|   month|day|hour|  weekday|\n",
      "+-------+-------------------+----+--------+---+----+---------+\n",
      "|      1|2017-01-03 00:00:00|2017| January|  3|  17|  Tuesday|\n",
      "|      2|2017-01-06 00:00:00|2017| January|  6|   5|   Friday|\n",
      "|      3|2017-01-31 00:00:00|2017| January| 31|   5|  Tuesday|\n",
      "|      4|2017-02-03 00:00:00|2017|February|  3|   6|   Friday|\n",
      "|      5|2017-02-08 00:00:00|2017|February|  8|   9|Wednesday|\n",
      "|      6|2017-02-11 00:00:00|2017|February| 11|  13| Saturday|\n",
      "|      7|2017-02-21 00:00:00|2017|February| 21|   9|  Tuesday|\n",
      "|      8|2017-02-25 00:00:00|2017|February| 25|   0| Saturday|\n",
      "|      9|2017-03-16 00:00:00|2017|   March| 16|   0| Thursday|\n",
      "|     10|2017-03-19 00:00:00|2017|   March| 19|  22|   Sunday|\n",
      "+-------+-------------------+----+--------+---+----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, to_date\n",
    "\n",
    "date_dim = bank_df.select('Year','full_date_time','Month','Day','Hour','WeekDay').distinct()\n",
    "date_dim = date_dim.withColumnRenamed('Year', 'year').withColumnRenamed('Month', 'month').withColumnRenamed('Day', 'day').withColumnRenamed('Hour', 'hour').withColumnRenamed('WeekDay', 'weekday')\n",
    "date_dim = date_dim.withColumn(\"date_id\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "date_dim = date_dim.select('date_id', 'full_date_time', 'year', 'month', 'day', 'hour','weekday')\n",
    "date_dim.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_dim.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+----------+---------------+------------+------------+-------+----------+----------+------------+-------------------+-----------+------------+-------+------+\n",
      "|     Id|Atm_Status|Currency|   Service|Transaction_Amt|Message_Code|Message_Text|Rain_3h|Clouds_All|Weather_Id|Weather_Main|Weather_Description|location_id|card_type_id|date_id|atm_id|\n",
      "+-------+----------+--------+----------+---------------+------------+------------+-------+----------+----------+------------+-------------------+-----------+------------+-------+------+\n",
      "| 572490|    Active|     DKK|Withdrawal|           2033|        null|        null|    0.0|        56|       701|        Mist|               mist|         50|          10|   1389|   104|\n",
      "|1875473|    Active|     DKK|Withdrawal|           1243|        null|        null|    0.0|        32|       802|      Clouds|   scattered clouds|         50|           5|   6134|   104|\n",
      "| 572489|    Active|     DKK|Withdrawal|           4502|        null|        null|    0.0|        56|       701|        Mist|               mist|         50|           5|   1389|   104|\n",
      "|1875731|    Active|     DKK|Withdrawal|           3863|        null|        null|    0.0|        32|       802|      Clouds|   scattered clouds|         50|           5|   6134|   104|\n",
      "| 572491|    Active|     DKK|Withdrawal|           6662|        null|        null|    0.0|        56|       701|        Mist|               mist|         50|           4|   1389|   104|\n",
      "|1875772|    Active|     DKK|Withdrawal|           2286|        null|        null|    0.0|        32|       802|      Clouds|   scattered clouds|         50|           5|   6134|   104|\n",
      "| 572631|    Active|     DKK|Withdrawal|           8094|        null|        null|    0.0|        32|       500|        Rain|         light rain|         50|           8|    423|   104|\n",
      "|1876285|    Active|     DKK|Withdrawal|           3636|        null|        null|    0.0|        32|       802|      Clouds|   scattered clouds|         50|           5|   6134|   104|\n",
      "| 572700|    Active|     DKK|Withdrawal|           5505|        null|        null|    0.0|        32|       701|        Mist|               mist|         50|           7|   2228|   104|\n",
      "|1876244|    Active|     DKK|Withdrawal|           2806|        null|        null|    0.0|        32|       802|      Clouds|   scattered clouds|         50|           4|   6134|   104|\n",
      "+-------+----------+--------+----------+---------------+------------+------------+-------+----------+----------+------------+-------------------+-----------+------------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fact Table \n",
    "df1 = bank_df_indexed\n",
    "df2 = location_dim\n",
    "df3 = card_type_dim\n",
    "df4 = date_dim\n",
    "df5 = atm_dim\n",
    "\n",
    "df1 = df1.alias('df1')\n",
    "df2 = df2.alias('df2')\n",
    "df3 = df3.alias('df3')\n",
    "df4 = df4.alias('df4')\n",
    "df5 = df5.alias('df5')\n",
    "\n",
    "bank_fact = df1.join(df2, (df1['Atm_location'] == df2['location']) & \n",
    "                        (df1['Atm_StreetName'] == df2['streetname']) & \n",
    "                        (df1['Atm_StreetNum'] == df2['street_number']) & \n",
    "                        (df1['Atm_Zipcode'] == df2['zipcode']) & \n",
    "                        (df1['Atm_Lat'] == df2['lat']) & \n",
    "                        (df1['Atm_Long'] == df2['lon'])).join(df3, df1['Card_type'] == df3['card_type']).join(df4, (df1['Year'] == df4['year']) & \n",
    "                           (df1['Month'] == df4['month']) & \n",
    "                           (df1['Day'] == df4['day']) & \n",
    "                           (df1['WeekDay'] == df4['weekday']) &\n",
    "                           (df1['Hour'] == df4['hour'])).join(df5, (df1['Atm_Id'] == df5['atm_number']) & \n",
    "                           (df1['Atm_Manufacturer'] == df5['atm_manufacturer']) & \n",
    "                           (df1['Atm_Location'] == df5['atm_location'])).select(df1['Id'], df1['Atm_Status'],\n",
    "                       df1['Currency'],df1['Service'],\n",
    "                       df1['Transaction_Amt'],df1['Message_Code'],\n",
    "                       df1['Message_Text'],df1['Rain_3h'],df1['Clouds_All'],\n",
    "                       df1['Weather_Id'],df1['Weather_Main'], df1['Weather_Description'],\n",
    "                       df2['location_id'], df3['card_type_id'], df4['date_id'], df5['atm_id'])\n",
    "bank_fact.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_fact=bank_fact.withColumnRenamed(\"Id\", \"trans_Id\").withColumnRenamed(\"location_id\", \"weather_loc_id\").withColumnRenamed(\"Atm_Status\", \"atm_status\").withColumnRenamed(\"Currency\", \"currency\").withColumnRenamed(\"Service\", \"service\").withColumnRenamed(\"Transaction_Amt\", \"transaction_amount\").withColumnRenamed(\"Message_Code\", \"message_code\").withColumnRenamed(\"Message_Text\", \"message_text\").withColumnRenamed(\"Rain_3h\", \"rain_3h\").withColumnRenamed(\"Clouds_All\", \"clouds_all\").withColumnRenamed(\"Weather_Id\", \"weather_id\").withColumnRenamed(\"Weather_Main\", \"weather_main\").withColumnRenamed(\"Weather_Description\", \"weather_description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_fact.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trans_Id',\n",
       " 'atm_status',\n",
       " 'currency',\n",
       " 'service',\n",
       " 'transaction_amount',\n",
       " 'message_code',\n",
       " 'message_text',\n",
       " 'rain_3h',\n",
       " 'clouds_all',\n",
       " 'weather_id',\n",
       " 'weather_main',\n",
       " 'weather_description',\n",
       " 'weather_loc_id',\n",
       " 'card_type_id',\n",
       " 'date_id',\n",
       " 'atm_id']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_fact.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
